import feedparser
import logging
from datetime import datetime
from time import mktime
from .config import Config

# Configura√ß√£o de Log local
logger = logging.getLogger(__name__)

def collect_feed_data(feed_url, limit=5):
    """
    Acessa um feed RSS e retorna uma lista de dicion√°rios com as not√≠cias.
    
    Args:
        feed_url (str): URL do RSS.
        limit (int): N√∫mero m√°ximo de not√≠cias para pegar.
    """
    logger.info(f"üîÑ Conectando ao feed: {feed_url}")
    
    try:
        # O feedparser baixa e analisa o XML automaticamente
        feed = feedparser.parse(feed_url)
        
        # Verifica se houve erro de conex√£o (bozo bit)
        if feed.bozo:
            logger.warning(f"‚ö†Ô∏è  Aviso de formato no feed {feed_url}: {feed.bozo_exception}")

        news_items = []
        
        # Itera sobre as not√≠cias (respeitando o limite)
        for entry in feed.entries[:limit]:
            # Tenta encontrar a data de publica√ß√£o (varia muito entre feeds)
            published_time = entry.get('published_parsed', entry.get('updated_parsed'))
            
            # Converte a data para um objeto Python utiliz√°vel
            pub_date = None
            if published_time:
                pub_date = datetime.fromtimestamp(mktime(published_time))
            
            # Cria um objeto limpo apenas com o que precisamos
            item = {
                'title': entry.get('title', 'Sem t√≠tulo'),
                'link': entry.get('link', ''),
                # Alguns feeds usam 'summary', outros 'description'
                'raw_summary': entry.get('summary', entry.get('description', '')),
                'published_at': pub_date
            }
            
            news_items.append(item)
            
        logger.info(f"‚úÖ {len(news_items)} not√≠cias coletadas de {feed_url}")
        return news_items

    except Exception as e:
        logger.error(f"‚ùå Erro cr√≠tico ao coletar {feed_url}: {e}")
        return []
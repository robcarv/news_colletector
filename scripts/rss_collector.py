import feedparser
import json
from datetime import datetime, timedelta
import os
import logging

# ConfiguraÃ§Ã£o de logs
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Lista de feeds RSS
RSS_FEEDS = [
    "https://feeds.folha.uol.com.br/emcimadahora/rss091.xml",
]

# FunÃ§Ã£o para coletar notÃ­cias de um feed
def collect_news(feed_url, max_news=5):
    feed = feedparser.parse(feed_url)
    news_items = []
    logger.info(f"\nğŸ” Processando feed: {feed_url}")
    logger.info(f"ğŸ“° Total de entradas encontradas: {len(feed.entries)}")

    for i, entry in enumerate(feed.entries):
        if is_today(entry.published_parsed):
            news_items.append({
                "title": entry.title,
                "summary": entry.summary if "summary" in entry else "",
                "link": entry.link,
                "source": feed.feed.title,
                "publication_date": entry.published if "published" in entry else ""
            })
            logger.info(f"âœ… NotÃ­cia {i+1} adicionada: {entry.title}")

            if len(news_items) >= max_news:
                logger.info(f"ğŸš« Limite de {max_news} notÃ­cias atingido para este feed.")
                break

    logger.info(f"ğŸ“¥ Total de notÃ­cias coletadas de {feed_url}: {len(news_items)}")
    return news_items

# FunÃ§Ã£o para verificar se a data de publicaÃ§Ã£o Ã© hoje
def is_today(published_parsed):
    if not published_parsed:
        logger.warning("âš ï¸ Aviso: Nenhuma data de publicaÃ§Ã£o encontrada para uma entrada.")
        return False
    published_date = datetime(*published_parsed[:6])
    today = datetime.now()
    return published_date.date() == today.date()

# FunÃ§Ã£o principal
def main():
    # Cria a pasta de dados se nÃ£o existir
    data_folder = "../data"
    if not os.path.exists(data_folder):
        os.makedirs(data_folder)
        logger.info(f"ğŸ“ Pasta de dados criada: {data_folder}")

    for feed_url in RSS_FEEDS:
        logger.info(f"\nğŸŒ Coletando notÃ­cias de: {feed_url}")
        news = collect_news(feed_url, max_news=10)

        # Salva as notÃ­cias coletadas em um arquivo JSON
        feed_name = feed_url.split("//")[1].split("/")[0].replace(".", "_")
        output_file = os.path.join(data_folder, f"{feed_name}_news.json")
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(news, f, ensure_ascii=False, indent=4)

        logger.info(f"ğŸ’¾ NotÃ­cias de {feed_url} salvas em {output_file}")
        logger.info(f"ğŸ“„ Total de notÃ­cias salvas para este feed: {len(news)}")

if __name__ == "__main__":
    main()